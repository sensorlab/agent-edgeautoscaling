# This section is only required for deploying Redis on Kubernetes for the purpose of enabling Ray
# to write GCS metadata to an external Redis for fault tolerance. If you have already deployed Redis
# on Kubernetes, this section can be removed.
kind: ConfigMap
apiVersion: v1
metadata:
  name: redis-config
  labels:
    app: redis
data:
  redis.conf: |-
    dir /data
    port 6379
    bind 0.0.0.0
    appendonly yes
    protected-mode no
    pidfile /data/redis-6379.pid
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  type: ClusterIP
  ports:
    - name: redis
      port: 6379
  selector:
    app: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      nodeSelector:
        kubernetes.io/hostname: jovyan-thinkpad-l14-gen-1
      containers:
        - name: redis
          image: redis:5.0.8
          command:
            - "sh"
            - "-c"
            - "redis-server /usr/local/etc/redis/redis.conf"
          ports:
            - containerPort: 6379
          volumeMounts:
            - name: config
              mountPath: /usr/local/etc/redis/redis.conf
              subPath: redis.conf
      volumes:
        - name: config
          configMap:
            name: redis-config
---
# Ray head node service, allowing worker pods to discover the head node to perform the bidirectional communication.
# More contexts can be found at [the Ports configurations doc](https://docs.ray.io/en/latest/ray-core/configure.html#ports-configurations).
apiVersion: v1
kind: Service
metadata:
  name: service-ray-cluster
  labels:
    app: ray-cluster-head
spec:
  clusterIP: None
  ports:
    - name: client
      protocol: TCP
      port: 10001
      targetPort: 10001
    - name: dashboard
      protocol: TCP
      port: 8265
      targetPort: 8265
    - name: gcs-server
      protocol: TCP
      port: 6380
      targetPort: 6380
  selector:
    app: ray-cluster-head
---
apiVersion: v1
kind: Pod
metadata:
  name: ray-head-pod
  labels:
    app: ray-cluster-head
    type: ray
    component: ray-head
spec:
  nodeSelector:
    kubernetes.io/hostname: jovyan-thinkpad-l14-gen-1
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
  containers:
    - name: ray-head
      image: rayproject/ray:2.9.0-py310-cpu
      imagePullPolicy: IfNotPresent
      resizePolicy:
        - resourceName: cpu
          restartPolicy: NotRequired
        - resourceName: memory
          restartPolicy: NotRequired
      command: [ "/bin/bash", "-c", "--" ]
      # if there is no password for Redis, set --redis-password=''
      args:
        - "ray start --num-cpus=4 --head --port=6380 --dashboard-host=0.0.0.0 --object-manager-port=8076 --node-manager-port=8077 --dashboard-agent-grpc-port=8078 --dashboard-agent-listen-port=52365 --redis-password='' --block"
      ports:
        - containerPort: 6380 # GCS server
        - containerPort: 10001 # Used by Ray Client
        - containerPort: 8265 # Used by Ray Dashboard
      # This volume allocates shared memory for Ray to use for its plasma
      # object store. If you do not provide this, Ray will fall back to
      # /tmp which cause slowdowns if it's not a shared memory volume.
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      env:
        # RAY_REDIS_ADDRESS lets ray use external Redis for fault tolerance
        - name: RAY_REDIS_ADDRESS
          value: redis:6379 # ip address for the external Redis, which is "redis:6379" in this example
        # This is used in the ray start command so that Ray can spawn the
        # correct number of processes. Omitting this may lead to degraded
        # performance.
        - name: MY_CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              resource: requests.cpu
      resources:
        limits:
          cpu: "2"
          memory: "4Gi"
        requests:
          cpu: "1"
          # not recommended allocating less than 2Gb memory for the Ray head pod.
          memory: "2Gi"
---
apiVersion: v1
kind: Pod
metadata:
  name: ray-worker-pod
  labels:
    component: ray-worker
    type: ray
    app: ray-cluster-worker
spec:
  nodeSelector:
    cluster: vm
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
  containers:
    - name: ray-worker
      image: rayproject/ray:2.9.0-py310-cpu
      resizePolicy:
        - resourceName: cpu
          restartPolicy: NotRequired
        - resourceName: memory
          restartPolicy: NotRequired
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: "1"
          memory: 1Gi
      imagePullPolicy: IfNotPresent
      command: [ "/bin/bash", "-c", "--" ]
      args:
        - "ray start --num-cpus=4 --address=service-ray-cluster:6380 --object-manager-port=8076 --node-manager-port=8077 --dashboard-agent-grpc-port=8078 --dashboard-agent-listen-port=52365 --block"
      # This volume allocates shared memory for Ray to use for its plasma
      # object store. If you do not provide this, Ray will fall back to
      # /tmp which cause slowdowns if it's not a shared memory volume.
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      env:
        # This is used in the ray start command so that Ray can spawn the
        # correct number of processes. Omitting this may lead to degraded
        # performance.
        - name: MY_CPU_REQUEST
          valueFrom:
            resourceFieldRef:
              resource: requests.cpu
        # The resource requests and limits in this config are too small for production!
        # It is better to use a few large Ray pods than many small ones.
        # For production, it is ideal to size each Ray pod to take up the
        # entire Kubernetes node on which it is scheduled.
